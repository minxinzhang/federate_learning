# 个人课程项目
用本地端口模拟分布式系统，可以理解为在多个小算力机器上跑本地小模型，通过网络将各本地模型传回服务器，服务器每次收到数据后更新全局模型再将全局模型反馈回分布式客户端。每个客户端的数据样本大小非均质，具体的模型训练原理及效果请阅读`FLReport.pdf`，涵盖了算法、各种梯度迭代的表现、某几个客户端离线后的模型训练效率。

## Requirements

This project requires:
1. Pytorch: 
https://pytorch.org/get-started/locally/
install method `python3 -m pip install torch`
2. Matplotlib: 
https://matplotlib.org/stable/users/getting_started/index.html#installation-quick-start
install method `python3 -m pip install matplotlib`

## How to run the demo

1. Run the server program first of all by either

`python3 COMP3221_FLServer.py 6000 0`
to disable subsampling mode or

`python3 COMP3221_FLServer.py 6000 1`
to enable subsampling of 2 devices. 

*Note that subsampling only exchanges 1 local model data if there's only one device connected to the server at a moment.*

After `COMP3221_FLServer.py` is running, you will see:
>[SERVER] Listening on port 6000<br>
>The server will use a non subsampling aggregation<br>
>[SERVER] Now please run client commands using Normal GD flag

2. Run client programs on different terminals by
`python3 COMP3221_FLClient.py <clientid> 600(x) [0|5|10|20]`

Follow the last prompt message from the server terminal to 
run clients as instructed.
For example:<br>
If you see 
>[SERVER] Now please run client commands using Normal GD flag

, run `python3 COMP3221_FLClient.py client1 6001 0` to launch 
the `client1` device.

Similarly,
>[SERVER] Now please run client commands using Mini Batch of 5 flag

run `python3 COMP3221_FLClient.py client1 6001 5` accordingly.

*Note that ::`clientid`:: is case sensitive. ::`clientid`:: is used to load data from the FLdata directory. ::`600(x)`:: is a placeholder to simulate the port number to which a client binds. My implementation let server handles socket pipes. You can use ::`6001`:: for all clients*

3. To achieve the graphical output generated by the "Matplotlib", you need to handle all 4 rounds of server instructions as

>[SERVER] Now please run client commands using Normal GD flag<br>
>[SERVER] Now please run client commands using Mini Batch of 5 flag<br>
>[SERVER] Now please run client commands using Mini Batch of 10 flag<br>
>[SERVER] Now please run client commands using Mini Batch of 20 flag<br>

*During each instruction round ::`clientid`:: logs terminal output under the same directory. E.g. ::`clientid_gd_log.txt`:: file for the GD simulation. ::`client1_batch_size_5_log.txt`:: for the Mini batch GD of size 5 simulation*


